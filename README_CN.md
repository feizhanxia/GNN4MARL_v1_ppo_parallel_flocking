# **ğŸ§  Equivariant RL Swarm â€” Parallel PPO for Collective Behavior**
[ç®€ä½“ä¸­æ–‡](README_CN.md) | [English](README.md)

![image](./image.png)


ä¸€ä¸ªé¢å‘ç¾¤ä½“æ™ºèƒ½ï¼ˆflocking/swarmingï¼‰ä»»åŠ¡çš„å¼ºåŒ–å­¦ä¹ å¹³å°ï¼Œæ”¯æŒåŸºäº GNN çš„ç­–ç•¥å­¦ä¹ ã€Vicsek æ¨¡å‹æ¯”è¾ƒã€å¤šè¿›ç¨‹å¹¶è¡Œé‡‡æ ·ã€ç²’å­ç³»ç»Ÿæ¨¡æ‹Ÿå’Œå¯è§†åŒ–ã€‚é¡¹ç›®ç»“æ„æ¸…æ™°ã€æ¨¡å—ç‹¬ç«‹ã€è°ƒè¯•å‹å¥½ï¼Œé€‚åˆç ”ç©¶è€…åœ¨æ­¤åŸºç¡€ä¸Šæ„å»ºæ›´å¤æ‚çš„ç¾¤ä½“æ„ŸçŸ¥ä¸æ§åˆ¶æ¨¡å‹ã€‚


## **ğŸ“ é¡¹ç›®ç»“æ„**

```zsh
tree --dirsfirst -I "*.pt|*.0|__pycache__|run*|image.png" -n -o tree.txt
```

```
.
â”œâ”€â”€ agents/                  # ç­–ç•¥å®šä¹‰
â”‚   â”œâ”€â”€ parallel_policy_ac.py       # GNN Actor-Critic policyï¼ˆæ”¯æŒ evaluate_actions + log_stdï¼‰
â”‚   â”œâ”€â”€ policy_base.py              # æ‰€æœ‰ç­–ç•¥çš„æŠ½è±¡æ¥å£
â”‚   â”œâ”€â”€ random_policy.py            # éšæœºç­–ç•¥ baseline
â”‚   â””â”€â”€ vicsek_policy.py            # Vicsek æ¨¡æ‹Ÿç­–ç•¥ baseline
â”‚
â”œâ”€â”€ env/                     # ç¯å¢ƒå®šä¹‰
â”‚   â”œâ”€â”€ base_env.py                  # åŸºç¡€ç²’å­æ›´æ–°é€»è¾‘
â”‚   â””â”€â”€ reward_env.py                # å¥–åŠ±å‡½æ•°å°è£…ï¼ˆå¯ç»„åˆæ¨¡å—ï¼‰
â”‚
â”œâ”€â”€ test/                    # å„ç­–ç•¥æµ‹è¯•è„šæœ¬
â”‚   â”œâ”€â”€ test_compare_parallel.py    # å¹¶åˆ—æµ‹è¯•ä¸‰ç§ç­–ç•¥å¹¶ç”ŸæˆåŠ¨ç”»
â”‚   â”œâ”€â”€ test_parallel_gnn_policy.py # åªè¿è¡Œè®­ç»ƒå¥½çš„ GNN ç­–ç•¥
â”‚   â”œâ”€â”€ test_random_policy.py       # è·‘ random baseline
â”‚   â””â”€â”€ test_vicsek_policy.py       # è·‘ Vicsek baseline
â”‚
â”œâ”€â”€ test_results/            # åŠ¨ç”»è¾“å‡º & reward æ›²çº¿
â”‚   â”œâ”€â”€ gnn_animation.mp4
â”‚   â”œâ”€â”€ random_animation.mp4
â”‚   â”œâ”€â”€ vicsek_animation.mp4
â”‚   â”œâ”€â”€ compare_rewards.png
â”‚   â””â”€â”€ flock_animation.mp4 
â”‚
â”œâ”€â”€ trainers/                # PPOè®­ç»ƒå™¨
â”‚   â””â”€â”€ parallel_ppo_trainer.py     # æ”¯æŒå¼‚æ­¥å¹¶è¡Œé‡‡æ ·å’Œæ¢¯åº¦æ›´æ–°
â”‚
â”œâ”€â”€ training_logs_parallel/  # TensorBoardæ—¥å¿—ç›®å½•
â”‚
â”œâ”€â”€ utils/                   # å·¥å…·åŒ…
â”‚   â”œâ”€â”€ evaluation_utils.py         # rolloutè¯„ä¼°å‡½æ•°
â”‚   â”œâ”€â”€ parallel_buffer.py          # å¤šè¿›ç¨‹ç»éªŒç¼“å­˜å™¨
â”‚   â”œâ”€â”€ training_utils.py           # å­¦ä¹ ç‡è°ƒåº¦ã€ç­–ç•¥è°ƒè¯•ç­‰
â”‚   â”œâ”€â”€ visualization_utils.py      # åŠ¨ç”»æ¸²æŸ“ã€rewardæ›²çº¿ç»˜åˆ¶
â”‚   â””â”€â”€ seed.py                     # å…¨å±€ç§å­æ§åˆ¶
â”‚
â”œâ”€â”€ README_CN.md
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt                # Python ä¾èµ–
â”œâ”€â”€ train_parallel_ppo_run.sh       # ä¸»è®­ç»ƒè„šæœ¬ï¼ˆzshï¼‰
â”œâ”€â”€ train_parallel_ppo.py           # ä¸»å…¥å£ï¼Œæ”¯æŒ argparse é…ç½®
â””â”€â”€ tree.txt                        # ç›®å½•ç»“æ„å¿«ç…§
```



## **ğŸš€ å¿«é€Ÿå¼€å§‹**

### **1. å®‰è£…ä¾èµ–**

```
pip install -r requirements.txt
```

æ­¤å¤–ï¼Œè¿˜éœ€è¦å®‰è£… PyTorch å’Œ Pytorch Geometricï¼Œå…·ä½“è¯·å‚è€ƒ [PyTorch å®˜ç½‘](https://pytorch.org/get-started/locally/) å’Œ [PyG å®˜ç½‘](https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html)ã€‚

### **2. å¯åŠ¨è®­ç»ƒï¼ˆæ¨èï¼‰**

```
bash train_parallel_ppo_run.sh
```

è¿™ä¸ªè„šæœ¬ä¸­è®¾ç½®äº†æ‰€æœ‰è¶…å‚æ•°å’Œç¯å¢ƒå˜é‡ï¼ˆagent æ•°é‡ã€box sizeã€radiusã€PPOå‚æ•°ç­‰ï¼‰ï¼Œå¯å¤ç°æ€§å¥½ã€‚

### **3. æŸ¥çœ‹æ—¥å¿—**

```
tensorboard --logdir training_logs_parallel
```

åŒ…å« reward æ›²çº¿ã€loss æ›²çº¿ã€åŠ¨ä½œå‡å€¼/stdã€log_std æ¼”åŒ–ç­‰ã€‚

### **4. ç­–ç•¥å¯¹æ¯” & åŠ¨ç”»ç”Ÿæˆ**

```
python test/test_compare_parallel.py
```

è¿è¡Œåä¼šåœ¨ test_results/ ä¸‹ç”Ÿæˆæ¯ä¸ªç­–ç•¥çš„å•ç‹¬åŠ¨ç”»ï¼ˆ.mp4ï¼‰å’Œ reward å¯¹æ¯”æ›²çº¿ã€‚

### **5. è¿è¡ŒæŸä¸€ç­–ç•¥è¿›è¡Œè§‚å¯Ÿ**

```
python test/test_parallel_gnn_policy.py
```



## **ğŸ¯ æ¨¡å‹ä¸è®­ç»ƒç‰¹ç‚¹**

- GNN Actor-Critic ç­–ç•¥ï¼Œæ”¯æŒ evaluate_actions() ä¸ log_prob() æ˜¾å¼è°ƒç”¨
- Learnable log_std + entropy lossï¼Œæœ‰æ•ˆé˜²æ­¢ç­–ç•¥åç¼©
- PPO è®­ç»ƒåŒ…æ‹¬ policy lossã€value lossã€entropy bonus
- GAE ä¼˜åŠ¿ä¼°è®¡ + reward å¹³æ»‘ + åŠ¨ä½œåˆ†å¸ƒæ­£åˆ™
- åŠ¨ä½œé™åˆ¶é€šè¿‡ TanhTransform æ˜ å°„è‡³ [-Ï€, Ï€]
- å¯è§†åŒ–åŒ…æ‹¬ï¼šåŠ¨ä½œæ–¹å‘ç®­å¤´ã€è½¨è¿¹ç²’å­åŠ¨ç”»ã€reward æ›²çº¿



## **âš™ï¸ å¹¶è¡Œè®­ç»ƒç»†èŠ‚**

- æ”¯æŒå¤šè¿›ç¨‹å¹¶è¡Œé‡‡æ ·ï¼ˆAsyncVectorEnv é£æ ¼ï¼‰
- æ¯ä¸ªé‡‡æ ·å™¨å…±äº«ç­–ç•¥æƒé‡ï¼Œä¸»è¿›ç¨‹ç»Ÿä¸€æ›´æ–°å‚æ•°
- ç»éªŒç¼“å­˜é‡‡ç”¨ ParallelBuffer ç»Ÿä¸€ç®¡ç†
- æ”¯æŒ ctrl+C å®‰å…¨ä¸­æ–­ï¼Œè‡ªåŠ¨ä¿å­˜å½“å‰æ¨¡å‹
- æ¯ N æ­¥ä¿å­˜ checkpointï¼Œå¯é€‰å¯ç”¨æœ€ä¼˜ç­–ç•¥ä¿å­˜æœºåˆ¶



## **ğŸ“„ ä¸»è¦æ–‡ä»¶**

- train_parallel_ppo_run.shï¼šæ ‡å‡†åŒ–å¯åŠ¨è„šæœ¬ï¼Œå®šä¹‰å®Œæ•´ config
- train_parallel_ppo.pyï¼šä¸»ç¨‹åºå…¥å£ï¼Œç»„ç»‡ç¯å¢ƒåˆå§‹åŒ–ã€ç­–ç•¥æ„å»ºã€trainer æ‰§è¡Œ
- parallel_ppo_trainer.pyï¼šPPO æ ¸å¿ƒé€»è¾‘ + å¤šçº¿ç¨‹ rollout ç®¡ç†
- parallel_policy_ac.pyï¼šç­–ç•¥ç½‘ç»œç»“æ„ï¼Œæ”¯æŒ actor/critic åˆ†å¼€ GCN ç¼–ç å™¨



## **âœ¨ TODO / å»¶å±•æ–¹å‘**

- æ”¯æŒ e3nn / SE(2) equivariant GNNï¼ˆå¯¹ç§°æ€§å¢å¼ºï¼‰
- å¤šå¤´ç­–ç•¥ç»“æ„ï¼ˆattention / role-basedï¼‰
- Curriculum / curriculum-aware PPO scheduler
- Grid ç¯å¢ƒç»“æ„æˆ–éšœç¢ç‰©å¹²æ‰°å»ºæ¨¡
- æ›´çœŸå®çš„ç²’å­äº’åŠ¨å»ºæ¨¡ï¼ˆç”µè·ã€æ„ŸçŸ¥ç›²åŒºç­‰ï¼‰



## **ğŸ§  æ³¨è®°**

è¿™ä¸ª repo èµ·åˆæ˜¯ä¸ºäº†ç ”ç©¶ç¾¤ä½“æ™ºèƒ½ä¸­ç²’å­å¦‚ä½•é€šè¿‡ GNN + RL è‡ªå‘å½¢æˆååŒè¡Œä¸ºã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€æ•´å¥—è®­ç»ƒæµæ°´çº¿ï¼Œä¸“æ³¨äºï¼š

- å®ç°ç¨³å®šçš„ PPO æ”¶æ•›
- åŠ¨ä½œç©ºé—´å¯¹ç§°æ€§ï¼ˆæ–¹å‘ã€ç¾¤ä½“ä½ç½®ï¼‰
- å¼ºè°ƒç­–ç•¥æ˜¯å¦â€œçœŸçš„å­¦åˆ°äº†ç»“æ„â€