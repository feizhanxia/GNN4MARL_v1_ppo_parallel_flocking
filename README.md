# **ğŸ§  Equivariant RL Swarm â€” Parallel PPO for Collective Behavior**
[ç®€ä½“ä¸­æ–‡](README_CN.md) | [English](README.md)

![image](./image.png)

A modular reinforcement learning framework for learning collective behavior (e.g. flocking) with GNN-based policies, Vicsek models, and random baselines. Designed with parallel sampling, visual diagnostics, stable PPO training, and swarm physics in mind â€” built for researchers and tinkerers alike.


## **ğŸ“ Project Structure**

```zsh
tree --dirsfirst -I "*.pt|*.0|__pycache__|run*|image.png" -n -o tree.txt
```

```
.
â”œâ”€â”€ agents/                  # Policy definitions
â”‚   â”œâ”€â”€ parallel_policy_ac.py       # GNN Actor-Critic policy (PPO-ready)
â”‚   â”œâ”€â”€ policy_base.py              # Abstract policy base class
â”‚   â”œâ”€â”€ random_policy.py            # Random movement baseline
â”‚   â””â”€â”€ vicsek_policy.py            # Vicsek-style heuristic policy
â”‚
â”œâ”€â”€ env/                     # Environment and reward logic
â”‚   â”œâ”€â”€ base_env.py                  # Core particle update logic
â”‚   â””â”€â”€ reward_env.py                # Modular reward definitions
â”‚
â”œâ”€â”€ test/                    # Evaluation scripts
â”‚   â”œâ”€â”€ test_compare_parallel.py    # Side-by-side animation & reward testing
â”‚   â”œâ”€â”€ test_parallel_gnn_policy.py # Run a trained GNN policy
â”‚   â”œâ”€â”€ test_random_policy.py       # Run the random baseline
â”‚   â””â”€â”€ test_vicsek_policy.py       # Run the Vicsek policy
â”‚
â”œâ”€â”€ test_results/            # Animation output and reward plots
â”‚   â”œâ”€â”€ gnn_animation.mp4
â”‚   â”œâ”€â”€ random_animation.mp4
â”‚   â”œâ”€â”€ vicsek_animation.mp4
â”‚   â”œâ”€â”€ compare_rewards.png
â”‚   â””â”€â”€ flock_animation.mp4
â”‚
â”œâ”€â”€ trainers/                # PPO training logic
â”‚   â””â”€â”€ parallel_ppo_trainer.py     # Parallel trainer using PPO
â”‚
â”œâ”€â”€ training_logs_parallel/  # TensorBoard logs
â”‚
â”œâ”€â”€ utils/                   # Utility modules
â”‚   â”œâ”€â”€ evaluation_utils.py         # Evaluation and rollout helpers
â”‚   â”œâ”€â”€ parallel_buffer.py          # Multi-agent rollout storage
â”‚   â”œâ”€â”€ training_utils.py           # Schedulers, diagnostics
â”‚   â”œâ”€â”€ visualization_utils.py      # Animation and plotting
â”‚   â””â”€â”€ seed.py                     # Reproducibility helpers
â”‚
â”œâ”€â”€ README_CN.md
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ train_parallel_ppo_run.sh       # Launch script with all hyperparams
â”œâ”€â”€ train_parallel_ppo.py           # Main entrypoint with argparse
â””â”€â”€ tree.txt                        # Project structure snapshot
```



## **ğŸš€ Getting Started**

### **1. Install dependencies**

```
pip install -r requirements.txt
```

Besides the standard libraries, **Pytorch** and **PyTorch Geometric** are required. Please install them according to your system configuration. 

**PyTorch dependencies:** [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)

**PyTorch Geometric dependencies:** [https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html](https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html)

### **2. Launch training (recommended)**

```
bash train_parallel_ppo_run.sh
```

This shell script defines and sets all hyperparameters (agent count, environment radius, GNN dim, PPO params, etc.) for a reproducible training run.

### **3. Launch TensorBoard**

```
tensorboard --logdir training_logs_parallel
```

Includes reward curves, policy/value loss, entropy evolution, log_std, etc.

### **4. Compare strategies & generate animations**

```
python test/test_compare_parallel.py
```

This will run all policies (GNN, Vicsek, Random), output individual animations, and generate a reward comparison plot in test_results/.

### **5. Run a policy manually**

```
python test/test_parallel_gnn_policy.py
```



## **ğŸ“ˆ Features**

- Learnable log_std with entropy-aware PPO
- GNN-based Actor-Critic policy with decoupled encoders
- Explicit evaluate_actions() and log_prob() handling
- PPO with GAE, clipping, entropy bonus, and value loss
- Actions mapped via TanhTransform to [-Ï€, Ï€]
- Trajectory visualization with velocity arrows
- Parallel rollout collection with shared policy weights
- Safe signal handling, checkpointing, and process cleanup
- Modular reward functions for physical constraints



## **âš™ï¸ Parallel Training Details**

- Uses multiprocessing for fast rollout collection
- Shared policy weights across workers
- Central trainer coordinates sampling & policy updates
- Asynchronous evaluation and policy checkpointing
- PPO steps batched across environment threads



## **ğŸ“„ Notable Files**

- train_parallel_ppo_run.sh: Shell script to launch training with all config flags
- train_parallel_ppo.py: Main launcher and CLI interface
- parallel_ppo_trainer.py: Training loop + policy updates
- parallel_policy_ac.py: Actor-Critic GNN architecture with log_std and evaluate support



## **ğŸ§ª Future Work & TODO**

- Equivariant GNN integration (e.g. e3nn / TFN)
- Attention-based policy (GATConv-style or Transformer head)
- Curriculum-based reward scaling
- Gridworld-like environment & obstacle modeling
- Energy-based or physics-inspired interaction rules



## **ğŸ§  Author Note**

This repo began as an exploration of whether GNNs and reinforcement learning can recreate collective intelligence from scratch. The current version emphasizes:

- Training stability in sparse reward settings
- Action-space symmetry handling
- Modular code for fast debugging and experimentation