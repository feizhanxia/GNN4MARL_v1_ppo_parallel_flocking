import torch
import os
from datetime import datetime
from torch.utils.tensorboard import SummaryWriter

def setup_training_directory(args):
    """è®¾ç½®è®­ç»ƒç›®å½•å’Œæ—¥å¿—"""
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    run_name = f"run_{timestamp}"
    save_dir = os.path.join(args.save_root, run_name)
    os.makedirs(save_dir, exist_ok=True)
    
    # ä¿å­˜é…ç½®
    with open(os.path.join(save_dir, "config.txt"), "w") as f:
        for k, v in vars(args).items():
            f.write(f"{k}: {v}\n")
        f.write("version: handcrafted_single_PPO\n")
    
    # è®¾ç½®TensorBoard
    writer = SummaryWriter(log_dir=os.path.join(save_dir, "tb"))
    print(f"âœ… TensorBoard logs will be saved to: {os.path.join(save_dir, 'tb')}")
    print("ğŸ‘‰ Run this anytime: tensorboard --logdir", args.save_root, "\n")
    
    return save_dir, writer

def save_model(save_dir, policy, config, filename, best_reward=None):
    """ä¿å­˜æ¨¡å‹"""
    save_dict = {
        'model_state_dict': policy.state_dict(),
        'config': config
    }
    if best_reward is not None:
        save_dict['best_reward'] = best_reward
    
    torch.save(save_dict, os.path.join(save_dir, filename))

def update_learning_rate(optimizer, current_episode, total_episodes, initial_lr, min_lr):
    """çº¿æ€§è¡°å‡å­¦ä¹ ç‡"""
    new_lr = max(min_lr, initial_lr * (1 - (current_episode + 1) / total_episodes))
    for param_group in optimizer.param_groups:
        param_group["lr"] = new_lr
    return new_lr 