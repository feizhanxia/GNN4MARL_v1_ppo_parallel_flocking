import torch
import argparse
import threading
from queue import Queue
import torch.multiprocessing as mp
import numpy as np
import signal
import sys
import time

from env.reward_env import FlockingEnv
from agents.parallel_policy_ac import ParallelGNNPolicyAC
from trainers.parallel_ppo_trainer import ParallelPPOTrainer
from utils.seed import set_seed
from utils.training_utils import setup_training_directory, save_model, update_learning_rate
from utils.evaluation_utils import Evaluator
from utils.visualization_utils import plot_reward_curve

# å…¨å±€å˜é‡ç”¨äºå­˜å‚¨å¥–åŠ±å†å²
reward_history = []

def signal_handler(sig, frame):
    """å¤„ç†ä¸­æ–­ä¿¡å·"""
    print("\næ­£åœ¨åœæ­¢è®­ç»ƒ...")
    sys.exit(0)

def worker_func(worker_id, args, policy_state_dict, sample_queue, stop_event):
    """å·¥ä½œè¿›ç¨‹å‡½æ•°"""
    try:
        # åˆ›å»ºæœ¬åœ°ç¯å¢ƒ
        env = FlockingEnv(
            n_agents=args.n_agents,
            box_size=args.box_size,
            radius=args.radius,
            dt=args.dt,
            speed=args.speed
        )
        
        # åˆ›å»ºæœ¬åœ°ç­–ç•¥ç½‘ç»œ
        device = torch.device("cpu")  # å·¥ä½œè¿›ç¨‹åªä½¿ç”¨CPU
        policy = ParallelGNNPolicyAC(
            in_dim=4,
            hidden_dim=args.hidden_dim,
            std=args.std,
            device=device
        )
        
        while not stop_event.is_set():
            # ä»å…±äº«å†…å­˜åŠ è½½ç­–ç•¥çŠ¶æ€
            policy.load_state_dict(dict(policy_state_dict))
            
            # æ‰§è¡Œä¸€ä¸ªepisodeçš„é‡‡æ ·
            obs = env.reset()
            states, actions, log_probs, rewards, values, dones, edge_indices = [], [], [], [], [], [], []
            
            for _ in range(args.steps_per_ep):
                x = torch.cat([obs['pos'], obs['vel']], dim=-1)
                edge_index = obs['edge_index']
                
                with torch.no_grad():
                    action, log_prob, value = policy.act(x, edge_index)
                
                obs, reward = env.step(action)
                
                # æ”¶é›†æ•°æ®
                states.append(x)
                actions.append(action)
                log_probs.append(log_prob)
                rewards.append(reward)
                values.append(value.squeeze(-1))
                dones.append(torch.zeros_like(reward))
                edge_indices.append(edge_index)
            
            # å°†æ•°æ®æ”¾å…¥é˜Ÿåˆ—
            episode_data = {
                'states': torch.stack(states),
                'actions': torch.stack(actions),
                'log_probs': torch.stack(log_probs),
                'rewards': torch.stack(rewards),
                'values': torch.stack(values),
                'dones': torch.stack(dones),
                'edge_index': edge_indices
            }
            sample_queue.put(episode_data)
            
            # æ¸…ç†å†…å­˜
            del states, actions, log_probs, rewards, values, dones, edge_indices
            del episode_data
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
            
    except Exception as e:
        print(f"å·¥ä½œè¿›ç¨‹ {worker_id} å‡ºé”™: {e}")
        raise

class ParallelSampler:
    def __init__(self, policy, args, n_workers=4):
        self.policy = policy
        self.args = args
        self.n_workers = n_workers
        self.device = torch.device("cuda" if torch.cuda.is_available() and args.device == "cuda" else "cpu")
        
        # åˆ›å»ºè¿›ç¨‹é—´é€šä¿¡é˜Ÿåˆ—ï¼Œé™åˆ¶å¤§å°ä¸ºå·¥ä½œè¿›ç¨‹æ•°çš„2å€
        self.sample_queue = mp.Queue(maxsize=n_workers * 2)
        self.stop_event = mp.Event()
        
        # åˆ›å»ºå…±äº«å†…å­˜æ¥å­˜å‚¨ç­–ç•¥çŠ¶æ€
        self.policy_state_dict = mp.Manager().dict()
        self.update_policy_state_dict(policy.state_dict())
        
        # åˆ›å»ºè¿›ç¨‹æ± 
        self.processes = []
        for i in range(n_workers):
            p = mp.Process(
                target=worker_func,
                args=(i, args, self.policy_state_dict, self.sample_queue, self.stop_event)
            )
            p.daemon = True
            self.processes.append(p)
    
    def update_policy_state_dict(self, new_state_dict):
        """æ›´æ–°å…±äº«å†…å­˜ä¸­çš„ç­–ç•¥çŠ¶æ€"""
        for k, v in new_state_dict.items():
            self.policy_state_dict[k] = v
    
    def update_policy(self, new_policy_state_dict):
        """æ›´æ–°å·¥ä½œè¿›ç¨‹çš„ç­–ç•¥ç½‘ç»œ"""
        self.update_policy_state_dict(new_policy_state_dict)
    
    def start(self):
        """å¯åŠ¨æ‰€æœ‰å·¥ä½œè¿›ç¨‹"""
        for p in self.processes:
            p.start()
    
    def stop(self):
        """åœæ­¢æ‰€æœ‰å·¥ä½œè¿›ç¨‹"""
        self.stop_event.set()
        for p in self.processes:
            if p.is_alive():
                p.terminate()
                p.join(timeout=1.0)
                if p.is_alive():
                    p.kill()
    
    def collect_samples(self, n_samples):
        """æ”¶é›†æŒ‡å®šæ•°é‡çš„æ ·æœ¬"""
        samples = []
        while len(samples) < n_samples:
            try:
                # è®¾ç½®è¾ƒçŸ­çš„è¶…æ—¶æ—¶é—´ï¼Œé¿å…é˜Ÿåˆ—ç§¯å‹
                sample = self.sample_queue.get(timeout=0.1)
                samples.append(sample)
            except:
                if self.stop_event.is_set():
                    break
                continue
        
        if not samples:
            raise RuntimeError("æ— æ³•æ”¶é›†åˆ°è¶³å¤Ÿçš„æ ·æœ¬")
        
        # åˆå¹¶æ‰€æœ‰æ ·æœ¬
        batch = {
            'states': torch.cat([s['states'] for s in samples]),
            'actions': torch.cat([s['actions'] for s in samples]),
            'log_probs': torch.cat([s['log_probs'] for s in samples]),
            'rewards': torch.cat([s['rewards'] for s in samples]),
            'values': torch.cat([s['values'] for s in samples]),
            'dones': torch.cat([s['dones'] for s in samples]),
            'edge_index': [edge_index for s in samples for edge_index in s['edge_index']]
        }
        
        # æ¸…ç†å†…å­˜
        del samples
        return batch

def run_parallel_ppo_training(args):
    # è®¾ç½®ä¸­æ–­ä¿¡å·å¤„ç†
    signal.signal(signal.SIGINT, signal_handler)
    
    # è®¾ç½®éšæœºç§å­å’Œè®¾å¤‡
    set_seed(args.seed)
    torch.set_num_threads(1)
    device = torch.device("cuda" if torch.cuda.is_available() and args.device == "cuda" else "cpu")
    
    # åˆ›å»ºæ¨¡å‹
    in_dim = 4
    policy = ParallelGNNPolicyAC(in_dim=in_dim, hidden_dim=args.hidden_dim, std=args.std, device=device)
    
    trainer = ParallelPPOTrainer(
        policy,
        gamma=args.gamma,
        lam=args.lam,
        lr=args.lr,
        radius=args.radius,
        clip_eps=args.clip_eps,
        vf_coef=args.vf_coef,
        ent_coef=args.ent_coef,
        epochs=args.epochs,
    )
    
    # è®¾ç½®è®­ç»ƒç›®å½•å’Œæ—¥å¿—
    save_dir, writer = setup_training_directory(args)
    
    # åˆå§‹åŒ–å¹¶è¡Œé‡‡æ ·å™¨
    sampler = ParallelSampler(policy, args, n_workers=args.n_workers)
    sampler.start()
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = Evaluator(policy, args)
    best_reward_queue = Queue()
    stop_event = threading.Event()
    
    # å¯åŠ¨è¯„ä¼°çº¿ç¨‹
    eval_thread = threading.Thread(
        target=evaluator.run_evaluation,
        args=(best_reward_queue, stop_event)
    )
    eval_thread.daemon = True
    eval_thread.start()
    
    try:
        for ep in range(args.episodes):
            # æ”¶é›†å¹¶è¡Œæ ·æœ¬
            batch = sampler.collect_samples(args.n_workers)
            
            # å°†æ•°æ®ç§»åˆ°è®­ç»ƒè®¾å¤‡
            batch = {
                'states': batch['states'].to(device),
                'actions': batch['actions'].to(device),
                'log_probs': batch['log_probs'].to(device),
                'rewards': batch['rewards'].to(device),
                'values': batch['values'].to(device),
                'dones': batch['dones'].to(device),
                'edge_index': batch['edge_index']  # edge_indexä¿æŒåŸæ ·ï¼Œå› ä¸ºå®ƒæ˜¯åˆ—è¡¨
            }
            
            # æ›´æ–°ç­–ç•¥
            policy_loss, value_loss, entropy_loss, total_loss = trainer.update_from_batch(batch)
            
            # æ›´æ–°å…±äº«å†…å­˜ä¸­çš„ç­–ç•¥
            sampler.update_policy(policy.state_dict())
            
            # æ›´æ–°å­¦ä¹ ç‡
            new_lr = update_learning_rate(trainer.optimizer, ep, args.episodes, args.lr, args.min_lr)
            
            # è®¡ç®—å¹³å‡å¥–åŠ±
            avg_reward = batch['rewards'].mean().item()
            avg_action = batch['actions'].mean().item()
            std_action = batch['actions'].std().item()
            
            # è®°å½•è®­ç»ƒä¿¡æ¯
            reward_history.append(avg_reward)
            writer.add_scalar("Loss/Total", total_loss, ep)
            writer.add_scalar("Loss/Policy", policy_loss, ep)
            writer.add_scalar("Loss/Value", value_loss, ep)
            writer.add_scalar("Loss/Entropy", entropy_loss, ep)
            writer.add_scalar("Average Reward", avg_reward, ep)
            writer.add_scalar("Action/Average", avg_action, ep)
            writer.add_scalar("Action/Std", std_action, ep)
            writer.add_scalar("Train/Learning Rate", new_lr, ep)
            writer.add_scalar("Policy/log_std", policy.log_std.item(), ep)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            while not best_reward_queue.empty():
                best_reward, best_state_dict = best_reward_queue.get()
                save_model(
                    save_dir,
                    policy,
                    {'in_dim': in_dim, 'hidden_dim': args.hidden_dim, 'std': args.std},
                    "policy_best.pt",
                    best_reward
                )
                print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ï¼Œå¥–åŠ±: {best_reward:.4f}")
            
            # æ‰“å°è®­ç»ƒä¿¡æ¯
            print(f"Ep {ep+1}/{args.episodes} | Reward: {avg_reward:.4f} | Loss: {total_loss:.4f}")
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if (ep + 1) % args.ckpt_interval == 0:
                save_model(
                    save_dir,
                    policy,
                    {'in_dim': in_dim, 'hidden_dim': args.hidden_dim, 'std': args.std},
                    f"policy_ep{ep+1}.pt"
                )
    
    except Exception as e:
        print(f"è®­ç»ƒå‡ºé”™: {e}")
        raise
    finally:
        stop_event.set()
        eval_thread.join()
        sampler.stop()
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹å’Œç»˜åˆ¶å¥–åŠ±æ›²çº¿
    save_model(
        save_dir,
        policy,
        {'in_dim': in_dim, 'hidden_dim': args.hidden_dim, 'std': args.std},
        "policy_final.pt"
    )
    plot_reward_curve(reward_history, save_dir, args.episodes)
    writer.close()
    print("\nâœ… Training complete. Final model saved.")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--n_agents", type=int, default=50)
    parser.add_argument("--box_size", type=float, default=10.0)
    parser.add_argument("--radius", type=float, default=1.0)
    parser.add_argument("--dt", type=float, default=0.1)
    parser.add_argument("--speed", type=float, default=1.0)
    parser.add_argument("--hidden_dim", type=int, default=64)
    parser.add_argument("--std", type=float, default=0.1)
    parser.add_argument("--min_std", type=float, default=1e-5)
    parser.add_argument("--gamma", type=float, default=0.99)
    parser.add_argument("--lam", type=float, default=0.95)
    parser.add_argument("--clip_eps", type=float, default=0.2)
    parser.add_argument("--vf_coef", type=float, default=0.5)
    parser.add_argument("--ent_coef", type=float, default=0.01)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--min_lr", type=float, default=1e-6, help="Minimum learning rate")
    parser.add_argument("--episodes", type=int, default=100)
    parser.add_argument("--steps_per_ep", type=int, default=300)
    parser.add_argument("--ckpt_interval", type=int, default=10)
    parser.add_argument("--device", type=str, choices=["cpu", "cuda"], default="cpu")
    parser.add_argument("--save_root", type=str, default="training_logs")
    parser.add_argument("--epochs", type=int, default=5)
    parser.add_argument("--seed", type=int, default=42, help="Random seed for reproducibility")
    parser.add_argument("--eval_interval", type=int, default=60, help="è¯„ä¼°é—´éš”ï¼ˆç§’ï¼‰")
    parser.add_argument("--eval_episodes", type=int, default=10, help="è¯„ä¼°æ¬¡æ•°")
    parser.add_argument("--n_workers", type=int, default=4, help="å¹¶è¡Œé‡‡æ ·å·¥ä½œè¿›ç¨‹æ•°")
    args = parser.parse_args()

    run_parallel_ppo_training(args) 